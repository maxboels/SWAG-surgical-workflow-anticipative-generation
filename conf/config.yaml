expt_name: "default"
# Just set to multiple values to run the same config multiple times. Just there
# to take into account random variation
run_id: 0
seed: 42
# A common place, so can be overriden in notebooks, which don't support ":"
# interpolation
cwd: ${hydra:runtime.cwd}
sync_bn: false
# Set this to force data parallel training. Num nodes should be 1.
data_parallel: true
dist_backend: nccl

# ----------------------Select parameters for experiments ----------------

# run mode
test_only: false

eval_horizons: [15, 30, 60] # in minutes

split_idx: 1 # 4-splits with indices [1, 2, 3, 4]

num_epochs: 20
best_score: 18.0
main_metric: "acc_curr_future" # wMAE
finetune_ckpt: best # or best

# dataset
dataset_name: autolaparo21 # cholec80 / autolaparo21
pooling_dim: 32
train_start: 1
train_end: 10
test_start: 15      
test_end: 21
save_video_labels_to_npy: false

# Comparison with other models
# 0. Trans-SVNeT (TSVN)
# TODO:

model_name: supra          # skit / supra / lstm / gru / naive1 / naive2
input_tokens: normal  # class_conditioned / normal
conditional_probs_embeddings: true
normalize_priors: false

decoder_anticipation: models.transformers.ClassConditionedTransformerDecoder

probs_to_regression_method: first_occurrence # first_occurrence / mean / median

naive1: false
naive2: false

do_classification: true
do_regression: true

base_rtd_loss: l1 # l1 / l2 / smooth_l1
rtd_weight_type: exponential # linear / exponential / inverse
rem_time_loss_fn: exponential # in_mae_zone_sensitive, horizon
rsd_loss_gamma: 4.0


ctx_pooling: global
num_ctx_tokens: 20
max_seq_len: 24
num_ant_queries: 30 # int(self.max_anticip_time * 60 / self.anticip_time)

# 1. SKiT
# base:     512  /  x   / x     /   x     /   x     /   true  / KeyRecorder

# SurGP (or SuPRA)
#        Informer / vocab / n_embd  / n_layer / n_head  /  infromer.decoder   /    Temp. Agg.
# base:     512  /  100   / 512     /   8     /   8     /   true              /   TokenPooler
# medium:   512  /  100   / 384     /   4     /   4     /   true              /   TokenPooler
# sm:       512  /  100   / 384     /   2     /   4     /   true              /   TokenPooler
# sm (3):   384  /  100   / 384     /   2     /   4     /   true              /   TokenPooler
# sm (2):   384  /  100   / 256     /   2     /   4     /   true              /   TokenPooler
# 9m:   512  /  100   / 384     /   2     /   8     /   false              /  TokenPooler
# s:        512  /  100   / 384     /   2     /   6     /   false             /   ...
# xs (ms):  512  /  100   / 384     /   2     /   4     /   false             /   ...
# xs (old): 512  /  100   / 128     /   2     /   2     /   true              /   ...
# xxs:      512  /  100   / 256     /   2     /   2     /   false             /   ...
# xxxs:     384  /  100   / 128     /   2     /   2     /   false             /   ...
informer:
  decoder: true             # default true
  d_model: 512
# future decoder: gpt-2 or transformer 
vocab_size: 1000            # base: 100, mv: 1000, lv: 10000
n_embd:     384
n_layer:    2
n_head:     4
fixed_ctx_length: true      # default true

mutli_token: false          # default false
# LABELS
num_curr_classes: 7         # 7 classes for cholec80 and autolaparo
num_next_classes: 8         # 7 classes for cholec80
num_future_classes: 8       # 7 classes for cholec80
eos_class: 7                # last class for EOS
eos_regression: false
eos_classification: true
eos_weight: count     # count or mean
class_weight: frequency # frequency / positional_inverse_frequency or positional: n-grames based on observed class and future prediction index.

# compression
ctx_length: 1440            # 1500s = 25min, 3000s = 50min (avg video is 34min)
# NOTE: we need long anticipation time to increase the number of transition samples for predicting transitions at inference.
anticip_time: 60            # training: 30s / 60s / 120s / 240s / 300s / 600s (10min)
max_anticip_time: 30        # inference: maximum duration (minutes)
# losses
feature_loss: false         # default false

gamma: 0.5                  # default 0.5 for the exponential weight decay
loss_w_curr: 0.5
loss_w_next: 0.5
loss_w_feats: 0.0
loss_w_remaining_time: 0.5
regression_loss_scale: 0.01



# ---------------------- end select params ----------------------------
train_eval_op:
    _target_: func.train_eval_ops.Basic
    cls_loss_acc_fn:
      _target_: func.train_eval_ops.BasicLossAccuracy
      base_rtd_loss: ${base_rtd_loss}
      rem_time_loss_fn: ${rem_time_loss_fn}
      rsd_loss_gamma: ${rsd_loss_gamma}
      weight_type: ${rtd_weight_type}
      gamma: ${gamma}
      loss_w_curr: ${loss_w_curr}
      loss_w_next: ${loss_w_next}
      loss_w_feats: ${loss_w_feats}
      loss_w_remaining_time: ${loss_w_remaining_time}
      regression_loss_scale: ${regression_loss_scale}

pytorch:
  # This only works with the compiled version of torchvision, and might have
  # some memory issues?
  video_backend: "video_reader"

train:
  fn: 'train'  # Which file in func/ directory to use for training
  batch_size: 64
  # This can have structure as follows:
  # <module name in model>:<module name in ckpt>:<path to ckpt> <>...
  # By default also supports just the <path to ckpt>
  # But the more complex structure can be used to init separate parts of model
  # using diff checkpoints. By default if only 2 elements are specified with :,
  # module_name_in_ckpt is assumed to be null
  init_from_model: null
  # Total epochs to train for
  num_epochs: ${num_epochs}
  # Evaluate within training, every these many epochs
  eval_freq: 1
  # Shuffle data at train time
  shuffle_data: true
  # Store the best performing checkpoint
  store_best: false
  train_one_epoch_fn:
    _target_: func.train.train_one_epoch
    print_freq: 10
    print_large_freq: 1000  # How often to write images/videos summary
    grad_clip_params: ${opt.grad_clip}  # DO NOT CHANGE HERE, change in opt
    # Set the following to store models every so many epochs. By default
    # will only store the last checkpoint and the best checkpoint.
    save_freq: null
    # Num of minutes to save at, same as above -- must set save_intermediate
    # true to save like this
    save_freq_min: 60  # At least save every 60 mins
    # Whether or not to save the intermediate models
    save_intermediates: false
    loss_wts:
      cls_one: 1.0
      pred: 1.0
      feat: 1.0
      # Past predictions, default 0 to be backward compatible
      past_cls_action: 0.0
      past_cls_verb: 0.0
      past_cls_noun: 0.0


eval:
  batch_size: 64
  eval_fn:
    _target_: func.train.evaluate
    eval_horizons: ${eval_horizons}
    anticip_time: ${anticip_time}
    max_anticip_time: ${max_anticip_time}
    store: false
    store_endpoint: logits
    only_run_featext: false
    probs_to_regression_method: ${probs_to_regression_method}
    confidence_threshold: 0.5
    do_classification: ${do_classification}
    do_regression: ${do_regression}


model:
  model_name: ${model_name}
  backbone_dim: 2048
  # Use the backbone dim if null. Don't use the interpolation since the
  # backbone dim might be updated in the code
  intermediate_featdim: null
  backbone_last_n_modules_to_drop: 2  # Avg pool and linear layer
  dropout: 0.0
  # Set to a number to project the temp_agg and future features to this
  # dimension using a MLP before applying the NCE loss.
  # Note this is also applied when doing L2 regression loss, so the name is a
  # bit of a misnomer.
  project_dim_for_nce: null
  # Set to true to also add a regression head -- that is used for dense
  # anticipation when predicting the duration of an action
  add_regression_head: False
  bn:
    eps: 0.001
    mom: 0.1
  # Set this to true if you want to have the same temporal aggregated feat
  # dim as from the original backbone (backbone_dim). This will add a linear
  # layer to get that. It's useful when training future predictive models,
  # with future feat avg as the target.
  same_temp_agg_dim: false
  # Set this to true to use the class mappings to get the other predictions
  # eg, verb/noun from action, instead of adding additional linear layers
  # Only applicable when predicting multiple output classes
  use_cls_mappings: false
  # Apply the classifier on the past predictions too
  classifier_on_past: false
  supra:
    _target_: models.supra.AVTh
    r2a2_model:
      _target_: R2A2.model.r2a2.R2A2
      naive1: ${naive1}
      naive2: ${naive2}
      dataset: ${dataset_name}
      mutli_token: ${mutli_token}
      feature_loss: ${feature_loss}
      decoder_type: ar_causal
      eos_regression: ${eos_regression}
      fixed_ctx_length: ${fixed_ctx_length}
      max_seq_len: ${max_seq_len}
      input_dim: 768
      present_length: 20
      num_ant_queries: ${num_ant_queries}
      past_sampling_rate: 10
      d_model: ${informer.d_model}
      num_curr_classes: ${num_curr_classes} # possible obseved classes
      num_next_classes: ${num_next_classes} # possible future classes (including EOS)
      max_anticip_time: ${max_anticip_time}
      anticip_time: ${anticip_time}
      pooling_dim: ${pooling_dim}
      ctx_pooling: ${ctx_pooling}
      num_ctx_tokens: ${num_ctx_tokens}
      gpt_cfg:
        vocab_size: ${vocab_size}
        n_embd: ${n_embd}
        n_layer: ${n_layer}
        n_head: ${n_head}
      key_recorder:
        _target_: R2A2.model.key_recorder.KeyRecorder
        dim: ${informer.d_model}
        reduc_dim: 64
        sampling_rate: 10
        local_size: 20
        pooling_method: max_abs
        return_max_now_reduc: true
        reduction: linear_relu
      fusion_head:
        _target_: R2A2.model.r2a2.FusionHead
        dim: ${informer.d_model}
      encoder:
        _target_: R2A2.model.r2a2.TransformerEncoder
        input_length: 20
        input_dim: 768
        d_model: 512
        n_heads: 4
        num_layers: 2
        dim_ff: 1024
        dropout: 0.3
        activation: relu
        reshape_output: false
      decoder:
        _target_: R2A2.model.r2a2.TransformerDecoder
        input_dim: 512
        d_model: 512
        n_heads: 4
        num_layers: 2
        dim_ff: 1024
        dropout: 0.1
        activation: relu
      informer:
        _target_: Informer2020.models.model.Informer
        enc_in: 768
        dec_in: 768
        c_out: 512
        seq_len: 96
        label_len: 48
        out_len: 1
        factor: 5
        d_model: ${informer.d_model}
        n_heads: 8
        e_layers: 2
        d_layers: 2
        d_ff: 1024
        dropout: 0.1
        attn: full
        embed: fixed
        freq: "h"
        activation: gelu
        output_attention: false
        distil: false
        mix: true
        use_decoder: ${informer.decoder}
  skit:
    _target_: models.skit_future.SKITFuture
    input_tokens: ${input_tokens}
    dataset: ${dataset_name}
    input_dim: 768
    present_length: 20
    past_sampling_rate: 10
    d_model: ${informer.d_model}
    dec_dim: ${n_embd}
    num_curr_classes: 7
    num_future_classes: ${num_future_classes}
    num_ant_queries: ${num_ant_queries}
    max_anticip_time: ${max_anticip_time}
    anticip_time: ${anticip_time}
    pooling_dim: 64
    ctx_pooling: ${ctx_pooling}
    num_ctx_tokens: ${num_ctx_tokens}
    do_classification: ${do_classification}
    do_regression: ${do_regression}
    fusion_head:
      _target_: R2A2.model.r2a2.FusionHead
      dim: ${informer.d_model}
    decoder:
      _target_: models.transformers.TransformerDecoder
      input_dim: ${informer.d_model}
      hidden_dim: ${n_embd}
      n_heads: ${n_head}
      n_layers: ${n_layer}
    decoder_cc:
      _target_: models.transformers.ClassConditionedTransformerDecoder
      cfg:
        dataset: ${dataset_name}
        horizon: ${max_anticip_time}
        num_classes: ${num_future_classes}
        do_classification: ${do_classification}
        do_regression: ${do_regression}
      num_queries: ${num_ant_queries}
      input_dim: ${informer.d_model}
      hidden_dim: ${n_embd}
      n_heads: ${n_head}
      n_layers: ${n_layer}
      conditional_probs_embeddings: ${conditional_probs_embeddings}
      normalize_priors: ${normalize_priors}
    informer:
      _target_: Informer2020.models.model.Informer
      enc_in: 768
      dec_in: 768
      c_out: 512
      seq_len: 96
      label_len: 48
      out_len: 1
      factor: 5
      d_model: ${informer.d_model}
      n_heads: 8
      e_layers: 2
      d_layers: 2
      d_ff: 1024
      dropout: 0.1
      attn: full
      embed: fixed
      freq: "h"
      activation: gelu
      output_attention: false
      distil: false
      mix: true
      use_decoder: ${informer.decoder}
  lstm:
    _target_: models.lstm.LSTM
    r2a2_model:
      _target_: R2A2.model.lstm_ar.LSTM
      mutli_token: ${mutli_token}
      feature_loss: ${feature_loss}
      decoder_type: ar_causal
      eos_regression: ${eos_regression}
      fixed_ctx_length: ${fixed_ctx_length}
      input_dim: 768
      present_length: 20
      num_ant_queries: 30
      past_sampling_rate: 10
      d_model: ${informer.d_model}
      num_curr_classes: ${num_curr_classes} # possible obseved classes
      num_next_classes: ${num_next_classes} # possible future classes (including EOS)
      max_anticip_time: ${max_anticip_time}
      anticip_time: ${anticip_time}
      pooling_dim: ${pooling_dim}
      ctx_pooling: ${ctx_pooling}
      num_ctx_tokens: ${num_ctx_tokens}
      gpt_cfg:
        vocab_size: ${vocab_size}
        n_embd: ${n_embd}
        n_layer: ${n_layer}
        n_head: ${n_head}
      key_recorder:
        _target_: R2A2.model.key_recorder.KeyRecorder
        dim: ${informer.d_model}
        reduc_dim: 64
        sampling_rate: 10
        local_size: 20
        pooling_method: max_abs
        return_max_now_reduc: true
        reduction: linear_relu
      fusion_head:
        _target_: R2A2.model.r2a2.FusionHead
        dim: ${informer.d_model}
      encoder:
        _target_: R2A2.model.r2a2.TransformerEncoder
        input_length: 20
        input_dim: 768
        d_model: 512
        n_heads: 4
        num_layers: 2
        dim_ff: 1024
        dropout: 0.3
        activation: relu
        reshape_output: false
      decoder:
        _target_: R2A2.model.r2a2.TransformerDecoder
        input_dim: 512
        d_model: 512
        n_heads: 4
        num_layers: 2
        dim_ff: 1024
        dropout: 0.1
        activation: relu
      informer:
        _target_: Informer2020.models.model.Informer
        enc_in: 768
        dec_in: 768
        c_out: 512
        seq_len: 96
        label_len: 48
        out_len: 1
        factor: 5
        d_model: ${informer.d_model}
        n_heads: 8
        e_layers: 2
        d_layers: 2
        d_ff: 1024
        dropout: 0.1
        attn: full
        embed: fixed
        freq: "h"
        activation: gelu
        output_attention: false
        distil: false
        mix: true
        use_decoder: ${informer.decoder}


opt:
  # Not using an overall LR anymore, since everything is now defined per
  # module.
  # Use a list format to specify per-layer LRs and WD. The first element is
  # module_name ("__all__" => all params), LR and WD.
  # Note that if there is any overlap between parameters, those params
  # will get updated that many number of times as they appear in the list.
  # It WILL NOT take the last options as highest precedence. (TODO future)
  # The first term can also be a list, to give it a bunch of modules to set
  # the same LR and WD for.
  lr_wd: [[__all__, 0.1, 0.0001]]
  # Set this to true to also scale the LR by the batch size (normally it will
  # be scaled by the #replicas, so the LR is specified per given batch size).
  # This allows to further specify a LR per batch element (useful when doing
  # sweeps over batch size).
  scale_lr_by_bs: false
  # Set this to true to only train the last classifier layer.
  # Also, will set all BN layers to not compute mean/var at runtime.
  classifier_only: false
  bias_bn_wd_scale: 1.0  # Scale the WD for bias and BN layers by this amount
  grad_clip:
    max_norm: null  # By default, no clipping
    norm_type: 2
  warmup:
    _target_: common.scheduler.Warmup
    init_lr_ratio: 0.0  # Warmup from this ratio of the orig LRs
    num_epochs: 0  # Warmup for this many epochs (will take out of total epochs)

moco:
  _target_: moco.moco.builder.MoCo
  dim: 128
  K: 65536
  m: 0.999
  T: 0.2  # From moco-v2
  mlp: true  # From moco-v2

defaults:
  - train_eval_op: basic
  - opt/optimizer: sgd
  - model/backbone: r2plus1d_34
  - model/temporal_aggregator: mean
  - model/future_predictor: identity
  - model/temporal_aggregator_after_future_pred: identity
  - model/classifier: linear
  - opt/scheduler: warmup_multi_step
  # Any keys with dataset_train prefix, like dataset_train2, etc, will all
  # be used for training by concatentating all those datasets. So you can
  # use multiple datasets in training by adding
  # +dataseset_train2=hmdb51/train to the command line config.
  # Note that this only works with standard datasets, ConcatDataset can't
  # handle overly customized datasets as we use in EpicKitchens
  # - dataset@dataset_train: epic_kitchens100/anticipation_train
  # Any keys with the dataset_eval prefix, will all be evaluated on separately.
  # The postfix will be used to identify which dataset the results are on.
  # So, you can use > 1 evaluation datasets that way, by adding it in the
  # command line config, like +dataset_eval2=hmdb51/val
  # - dataset@dataset_eval: epic_kitchens100/anticipation_val
  - data@data_train: default
  - data@data_eval: default
  # Load any common dataset files, that will be used to create other dataset
  # elements.
  # - dataset/epic_kitchens/common
  # - dataset/epic_kitchens100/common
  # - dataset/dundee50salads/common
  # - dataset/dundee50salads/annot_reader_fn: orig
  # - dataset/egtea/common
  # Overrides
  - override hydra/launcher: submitit_slurm
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog

hydra:
  job:
    name: "AVT"
  launcher:
    # All params in https://github.com/facebookresearch/hydra/blob/master/plugins/hydra_submitit_launcher/hydra_plugins/hydra_submitit_launcher/config.py
    timeout_min: 2880
    cpus_per_task: 10
    gpus_per_node: 8
    tasks_per_node: ${hydra.launcher.gpus_per_node}
    # This is the memory requested per node. So all GPUs on a given
    # node will share this memory
    mem_gb: 450
    nodes: 1
    # Use these parameters through + options in hydra
    # partition: learnfair
    # max_num_timeout: 3
    # constraint: ${hydra.launcher.gpu_type} # Any, or could say [volta|pascal]
    # comment: ""
  run:
    dir: ./outputs/  # Specified in the launch script
  sweep:
    dir: ${hydra.run.dir}
    # Output sub directory for sweep runs.
    subdir: ${hydra.job.num}  # ${hydra.job.override_dirname}
